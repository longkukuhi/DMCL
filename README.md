<h1 align="center">
ğŸ§ Towards Hallucination-Robust Diffusion Augmented Interactive
Text-to-Image Retrieval</h1>

<p align = "center">
<img src="assets/arch.png">
</p>


* **Official PyTorch implementation for paper:  "Towards Hallucination-Robust Diffusion Augmented Interactive Text-to-Image Retrieval "** <br>

## ğŸ“° Updates
- [2026/01/27] The code of DMCL is released! ğŸ‰

## ğŸ—ï¸ Table of Contents
- [Setup](#-setup)
- [Download Pretrained Weights](#-download-the-beit-3-pretrain-weight-for-retrieval-task)
- [Data Preparation](#-data-preparation)
- [Download Checkpoints](#-download-our-checkpoints)
- [Training and Evaluation](#-training-and-evaluation)
- [Acknowledgements](#-acknowledgements)
- [License](#-license)
- [Citation](#-citation)

## ğŸ› ï¸ Setup
First, clone this repository to your local machine, and install the dependencies.
```bash
pip install -r requirements.txt
```
â— You can modify the PyTorch version to suit your machine.

## â¬‡ï¸ Download The BEiT-3 Pretrain Weight for Retrieval Task
This project relies on the official Microsoft BEiT-3 implementation and pretrained weights.
Download the pretrain model weights and the tokenizer model for retrieval task.
   - [`BEiT3-base-itc`](https://github.com/microsoft/unilm/tree/master/beit3#pretrained-models): #layer=12; hidden=768; FFN factor=4x; #head=12; patch=16x16; #parameters: 222M
   - [`beit3.spm`](https://github.com/microsoft/unilm/tree/master/beit3#pretrained-models): the sentencepiece model used for tokenizing texts.

## ğŸ’¾ Data Preparation
Please download the required datasets from the following sources:
   - [`VisDial v1.0`](https://visualdialog.org/): The foundational dataset used for constructing our training data training and evaluation, consisting of images and multi-turn dialogues.
   - [`DAI-TIR Dataset`](https://drive.google.com/drive/folders/1JhXEoeiuwKNsVlm6LdJXFcbxYMcaJTw6?usp=sharing): This dataset contains the diffusion-augmented training samples described in our paper.The dataset is currently being prepared for release. We will update this section with the download link upon acceptance.
   - [`ChatIR Benchmark`](https://github.com/levymsn/ChatIR?tab=readme-ov-file#table-of-contents): Four eval benchmark diglogue dataset with diverse dialogue styles (e.g., ChatGPT, Human).
   - [`PlugIR`](https://github.com/Saehyung-Lee/PlugIR): A dataset featuring concise, summary-style queries generated via an interactive pipeline.

## ğŸ—‚ï¸ Download our checkpoints
The checkpoints will be made publicly available upon acceptance of the paper.

## ğŸš€ Training and Evaluation

###  Directory Structure

To ensure the code runs correctly, you can organize your project directory as follows. Alternatively, you can modify the paths in `dmcl_config.py` to match your custom directory structure.

```text
.
â”œâ”€â”€ beit3/                      # Official Microsoft BEiT-3 code
â”‚   â”œâ”€â”€ modeling_finetune.py    # Core modeling code
â”‚   â”œâ”€â”€ optim_factory.py        # Optimizer utilities
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                       # Dataset root directory
â”‚   â”œâ”€â”€ visdial_1.0_train.json  # VisDial v1.0 training file
â”‚   â”œâ”€â”€ query_images/           # Training reference images (DA-VisDial)
â”‚   â”‚   â”œâ”€â”€ train-xxxx_0.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ ChatIR_Protocol/        # Validation Corpus
â”‚   â”‚   â””â”€â”€ Search_Space_val_50k.json
â”‚   â”œâ”€â”€ dialogues/              # Validation Queries
â”‚   â”‚   â””â”€â”€ VisDial_v1_0_queries_val.json
â”‚   â””â”€â”€ generated_images/       # Pre-generated images for validation
â”‚       â””â”€â”€ VisDial_v1_0_queries_val/
â”‚           â””â”€â”€ your_generated_images/
â”œâ”€â”€ model/                      # Pretrained Checkpoints
â”‚   â”œâ”€â”€ beit3_base_itc_patch16_224.pth
â”‚   â””â”€â”€ beit3.spm
â”œâ”€â”€ dmcl_config.py
â”œâ”€â”€ train.py
â”œâ”€â”€ eval_dmcl.py
â”œâ”€â”€README.md
â””â”€â”€ ...
```

### Training
You can adjust the training hyperparameters by passing command-line arguments. Alternatively, you can configure them directly by modifying dmcl_config.py, allowing you to simply run:
```bash
python train.py
```

### Evaluation
To perform a complete evaluation of the experiment, run the following command:
```bash
python eval_dmcl.py
```

## ğŸ¤ Acknowledgements

Our code is built upon the excellent work of [Microsoft BEiT-3](https://github.com/microsoft/unilm/tree/master/beit3). We thank the authors for their open-source contribution.

We also express our gratitude to the following projects for providing datasets and evaluation protocols:
* [VisDial v1.0](https://visualdialog.org/) for the visual dialogue dataset.
* [ChatIR](https://github.com/levymsn/ChatIR) and [PlugIR](https://github.com/Saehyung-Lee/PlugIR) for the interactive text-to-image benchmarks and baselines.

## âš–ï¸ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

## âœï¸ Citation

If you find this code useful for your research, please consider citing our paper:







